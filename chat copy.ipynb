{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.xlnet.modeling_tf_xlnet import TFXLNetLMHeadModel\n",
    "CUSTOMER_TOKENS = [12967, 30]\n",
    "SALES_TOKENS = [4925, 30]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-xlnet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLNetLMHeadModel.\n",
      "\n",
      "All the layers of TFXLNetLMHeadModel were initialized from the model checkpoint at /home/yangkaixuan/project/mama/mymodel6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# model = TFXLNetLMHeadModel.from_pretrained(\"/datafile/kaixuan/nlg/mymodel6s\")\n",
    "model = TFXLNetLMHeadModel.from_pretrained(\"/home/yangkaixuan/project/mama/mymodel6\")\n",
    "model.transformer.attn_type = 'bi'\n",
    "# model = TFXLNetLMHeadModel.from_pretrained(\"hfl/chinese-xlnet-base\")\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [\"很高兴服务到您\",\n",
    "           \"我是鹤壁市浚县小河镇的，我们附近有没有你们的业务员？\",\n",
    "           \"在鹤壁市的话暂时没有分公司，现在在江苏，河北，还有山东是有分公司的，其实您问有没有分公司，也是担心后期理赔这方面对吧？\",\n",
    "           ]\n",
    "inputs=[]\n",
    "customer_i=[1,3,4,5]\n",
    "for i,text in enumerate(contexts):\n",
    "    if i in customer_i:\n",
    "        inputs.extend(CUSTOMER_TOKENS)\n",
    "    else:\n",
    "        inputs.extend(SALES_TOKENS)\n",
    "    inputs.extend(tokenizer.encode(text,add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['客户我再问您,目前有没有分公司是担心后期服务这方面对客户服务',\n",
       " '客户我担心后期服务 后期服务 后期服务 客户我再考虑 德安',\n",
       " '客户我担心后期理服务,担心后期服务。 客户我担心后期服务,担心',\n",
       " '客户我再考虑吧 销售 您考虑是考虑还是考虑 客户我再考虑',\n",
       " '客户我担心后期服务,担心后期后期服务。 担心后期服务,担心后期服务',\n",
       " '客户我担心后期服务,担心后期服务。 担心后期服务,担心后期服务。',\n",
       " '客户我担心后期服务,担心后期后期服务。 担心后期服务,担心后期服务',\n",
       " '客户我再考虑 销售 您考虑是考虑还是考虑 客户我再考虑 ',\n",
       " '客户我再考虑 销售 您考虑是考虑还是考虑 客户我再考虑 ',\n",
       " '客户我再考虑 销售 您考虑是考虑还是考虑 客户我再考虑 ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"我再考虑一下\"\n",
    "inputs1=inputs+CUSTOMER_TOKENS+tokenizer.encode(question,add_special_tokens=False)+SALES_TOKENS\n",
    "n=len(inputs1)\n",
    "print(n)\n",
    "inputs1=tf.convert_to_tensor(\n",
    "    inputs1, dtype=None, dtype_hint=None, name=None\n",
    ")\n",
    "inputs1=inputs1[None,:]\n",
    "# no_repeat_ngram_size=2,\n",
    "# num_beams=5,\n",
    "outputs = model.generate(inputs1,temperature=0.7, repetition_penalty =1.1, num_beams=5,\n",
    "                         max_length=n+20, do_sample=True, top_p=0.95,top_k=30,num_return_sequences=20)\n",
    "generateds=[]\n",
    "for i, output in enumerate(outputs):\n",
    "    generated = tokenizer.decode(output[n:], skip_special_tokens=False)\n",
    "    generateds.append(generated)\n",
    "generateds.sort(key=lambda x :len(set(x)), reverse=True)\n",
    "generateds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['没的姐我理解的意思其实现在保险东西都是合同的合同,都是法律的您心',\n",
    " '那,也问题,您边有虑担心,是原因?担心理方面还是?公司',\n",
    " '那您边有需要可以小这给介绍下 有我您边好了解 给',\n",
    " '您边问题了不给发了?哪不心呀 觉得骗?还是 啥',\n",
    " '好的,事给讲下,看有理服务的 理方面比较一些 有的',\n",
    " '是,这个完全担心  不您我理解 不我您了解 您相信就',\n",
    " '咱是总公司,是赔您的,您合同给邮到手中,这您放,',\n",
    " '您看下您有需求我您边边给发就,这产品的费就高',\n",
    " '您心有数肯定知道现在人子,有骗的,您对也心数是',\n",
    " '没姐我们在边都是的 正规保险 赔    放就  ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['咱德安人总公司山东              ',\n",
       " '德安人总公司山东               ',\n",
       " '德安人总公司山东               ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"你是哪里的朋友\"\n",
    "inputs1=inputs+CUSTOMER_TOKENS+tokenizer.encode(question,add_special_tokens=False)+SALES_TOKENS\n",
    "n=len(inputs1)\n",
    "print(n)\n",
    "inputs1=tf.convert_to_tensor(\n",
    "    inputs1, dtype=None, dtype_hint=None, name=None\n",
    ")\n",
    "inputs1=inputs1[None,:]\n",
    "outputs = model.generate(inputs1,num_beams=10, repetition_penalty =1.2,\n",
    "                         max_length=n+20, do_sample=True, top_p=0.95, num_return_sequences=3)\n",
    "generateds=[]\n",
    "for i, output in enumerate(outputs):\n",
    "    generated = tokenizer.decode(output[n:], skip_special_tokens=True)\n",
    "    generateds.append(generated)\n",
    "generateds.sort(key=lambda x :len(set(x)), reverse=True)\n",
    "generateds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 56, 3285, 1937, 4, 3]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"5块钱\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'與'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
    "        beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n",
    "\n",
    "        Adapted in part from `Facebook's XLM beam search code\n",
    "        <https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529>`__.\n",
    "\n",
    "        Apart from :obj:`input_ids` and :obj:`attention_mask`, all the arguments below will default to the value of the\n",
    "        attribute of the same name inside the :class:`~transformers.PretrainedConfig` of the model. The default values\n",
    "        indicated are the default values of those config.\n",
    "\n",
    "        Most of these parameters are explained in more detail in `this blog post\n",
    "        <https://huggingface.co/blog/how-to-generate>`__.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            input_ids (:obj:`tf.Tensor` of :obj:`dtype=tf.int32` and shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it with\n",
    "                :obj:`bos_token_id` and a batch size of 1.\n",
    "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
    "                The maximum length of the sequence to be generated.\n",
    "            min_length (:obj:`int`, `optional`, defaults to 10):\n",
    "                The minimum length of the sequence to be generated.\n",
    "            do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "            early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
    "            num_beams (:obj:`int`, `optional`, defaults to 1):\n",
    "                Number of beams for beam search. 1 means no beam search.\n",
    "            temperature (:obj:`float`, `optional`, defaults to 1.0):\n",
    "                The value used to module the next token probabilities.\n",
    "            top_k (:obj:`int`, `optional`, defaults to 50):\n",
    "                The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "            top_p (:obj:`float`, `optional`, defaults to 1.0):\n",
    "                If set to float < 1, only the most probable tokens with probabilities that add up to ``top_p`` or\n",
    "                higher are kept for generation.\n",
    "            repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
    "                The parameter for repetition penalty. 1.0 means no penalty. See `this paper\n",
    "                <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.\n",
    "            pad_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `padding` token.\n",
    "            bos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `beginning-of-sequence` token.\n",
    "            eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the `end-of-sequence` token.\n",
    "            length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
    "                Exponential penalty to the length. 1.0 means no penalty.\n",
    "\n",
    "                Set to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in\n",
    "                order to encourage the model to produce longer sequences.\n",
    "            no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
    "                If set to int > 0, all ngrams of that size can only occur once.\n",
    "            bad_words_ids(:obj:`List[int]`, `optional`):\n",
    "                List of token ids that are not allowed to be generated. In order to get the tokens of the words that\n",
    "                should not appear in the generated text, use :obj:`tokenizer.encode(bad_word, add_prefix_space=True)`.\n",
    "            num_return_sequences(:obj:`int`, `optional`, defaults to 1):\n",
    "                The number of independently computed returned sequences for each element in the batch.\n",
    "            attention_mask (:obj:`tf.Tensor` of :obj:`dtype=tf.int32` and shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for\n",
    "                tokens that are not masked, and 0 for masked tokens.\n",
    "\n",
    "                If not provided, will default to a tensor the same shape as :obj:`input_ids` that masks the pad token.\n",
    "\n",
    "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "            decoder_start_token_id (:obj:`int`, `optional`):\n",
    "                If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.\n",
    "            use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
    "                speed up decoding.\n",
    "            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
    "            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "            forced_bos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the token to force as the first generated token after the :obj:`decoder_start_token_id`.\n",
    "                Useful for multilingual models like :doc:`mBART <../model_doc/mbart>` where the first generated token\n",
    "                needs to be the target language token.\n",
    "            forced_eos_token_id (:obj:`int`, `optional`):\n",
    "                The id of the token to force as the last generated token when :obj:`max_length` is reached.\n",
    "            model_specific_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model.\n",
    "\n",
    "        Return:\n",
    "            :class:`~transformers.file_utils.ModelOutput` or :obj:`tf.Tensor`: A\n",
    "            :class:`~transformers.file_utils.ModelOutput` (if ``return_dict_in_generate=True`` or when\n",
    "            ``config.return_dict_in_generate=True``) or a :obj:`tf.Tensor`.\n",
    "\n",
    "                If the model is `not` an encoder-decoder model (``model.config.is_encoder_decoder=False``), the\n",
    "                possible :class:`~transformers.file_utils.ModelOutput` types are:\n",
    "\n",
    "                    - :class:`~transformers.generation_utils.TFGreedySearchDecoderOnlyOutput`,\n",
    "                    - :class:`~transformers.generation_utils.TFSampleDecoderOnlyOutput`,\n",
    "                    - :class:`~transformers.generation_utils.TFBeamSearchDecoderOnlyOutput`,\n",
    "                    - :class:`~transformers.generation_utils.TFBeamSampleDecoderOnlyOutput`\n",
    "\n",
    "                If the model is an encoder-decoder model (``model.config.is_encoder_decoder=True``), the possible\n",
    "                :class:`~transformers.file_utils.ModelOutput` types are:\n",
    "\n",
    "                    - :class:`~transformers.generation_utils.TFGreedySearchEncoderDecoderOutput`,\n",
    "                    - :class:`~transformers.generation_utils.TFSampleEncoderDecoderOutput`,\n",
    "                    - :class:`~transformers.generation_utils.TFBeamSearchEncoderDecoderOutput`,\n",
    "                    - :class:`~transformers.generation_utils.TFBeamSampleEncoderDecoderOutput`\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n",
    "            model = TFAutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from huggingface.co and cache.\n",
    "            outputs = model.generate(max_length=40)  # do greedy decoding\n",
    "            print(f'Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}')\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained('openai-gpt')   # Initialize tokenizer\n",
    "            model = TFAutoModelWithLMHead.from_pretrained('openai-gpt')    # Download model and configuration from huggingface.co and cache.\n",
    "            input_context = 'The dog'\n",
    "            input_ids = tokenizer.encode(input_context, return_tensors='tf')  # encode input context\n",
    "            outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n",
    "            for i in range(3): #  3 output sequences were generated\n",
    "                print(f'Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}')\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n",
    "            model = TFAutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from huggingface.co and cache.\n",
    "            input_context = 'The dog'\n",
    "            input_ids = tokenizer.encode(input_context, return_tensors='tf')  # encode input context\n",
    "            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3, do_sample=True)  # generate 3 candidates using sampling\n",
    "            for i in range(3): #  3 output sequences were generated\n",
    "                print(f'Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}')\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained('ctrl')   # Initialize tokenizer\n",
    "            model = TFAutoModelWithLMHead.from_pretrained('ctrl')    # Download model and configuration from huggingface.co and cache.\n",
    "            input_context = 'Legal My neighbor is'  # \"Legal\" is one of the control codes for ctrl\n",
    "            input_ids = tokenizer.encode(input_context, return_tensors='tf')  # encode input context\n",
    "            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences\n",
    "            print(f'Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}')\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained('gpt2')   # Initialize tokenizer\n",
    "            model = TFAutoModelWithLMHead.from_pretrained('gpt2')    # Download model and configuration from huggingface.co and cache.\n",
    "            input_context = 'My cute dog'\n",
    "            bad_words_ids = [tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ['idiot', 'stupid', 'shut up']]\n",
    "            input_ids = tokenizer.encode(input_context, return_tensors='tf')  # encode input context\n",
    "            outputs = model.generate(input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids)  # generate sequences without allowing bad_words to be generated\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "649de5dbb7f4632d1be21c829ba0eb34ab25e1f7d7869e9793807b1ab89a1aac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf2.6': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
